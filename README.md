# From Automation to Testing

*Learn what it means to start testing rather than just automating.*

Automation vs Testing |
---- |
Automation != Testing |
Automation is in the service of testing, not vice versa. And it's possible to use automation towards unhelpful goals |
We need to think of our job as testers first and automators second, even if much of our time is spent writing automation |

Automation is not the same as testing, and the purpose of learning all this automation is to enable oneself to become a better software tester! So let's talk about how we move from automation to testing, both philosophically and practically.

Even though today software testing often involves automation, and automation is often implemented for the purpose of testing software, the two concepts are actually logically unrelated. Still, it can be hard not to conflate them. And the danger in conflating them is real, because it can lead us to focus on automation and forget that the real name of the game here is testing. At the end of the day, automation is a means to this end, as fun and satisfying as it may be in its own right. Just because we have figured out how to automate every last corner of our app does not necessarily imply that we've written a single good test. As we talked about early on in the course, testing is the process of building the right kind of crucible for our app. We can put automation to use in any way we want, even towards the wrong goals, the wrong kinds of crucibles for our app. So it's always important to remember that we need to think about our app as testers first, and automators second, even if 80-90% of our time still ends up being spent writing automation code.

Thinking Like a Tester |
---- |
Know your app inside and out from a user perspective. Be a power user of your own app. |
Know your users' desires, intentions, and frustrations. You are a gatekeeper of app quality from a product design perspective--not just a bug finder. |
Empathize with users to find edge cases product designers might have missed thinking about. But know which edge cases are ones to care about and which aren't. |
Ensure total clarity of test requirements (requirements will often be given to you vaguely, and it's up to you to clarify) |
Remember that you are not a robot. Your job is to make robots do useful things, not to become a robot yourself. |

So, how do you think like a tester first, to ensure that your automation is put to good use? Here are a number of pointers I'd encourage you to keep in mind:

1. First, you need to know your product. You need to understand your app, the problem it's trying to solve, and the intentions of the designers and developers that resulted in the artifact you have to test. If you don't thoroughly understand your app, you won't know what it means to test it, and you certainly won't be able to empathize with the users who end up with the app.
2. Second, on that note, you need to know your users. Who is using your app? What are they trying to get done with it? What are their desires and intentions? Ideally, the app has been built with the user in mind, and the product managers have thought through the user's needs very deeply, building the app to directly meet their needs. But this is not always the case. It sometimes happens that there are subtle disconnects between the product and its users, which you as a tester can help to surface. In a good team, every single team member takes responsibility for the product's ability to meet the user's needs. As a tester, you are often the last line of defense for your user, with the ability to flag potential defects or even features that just haven't been developed in the right direction. Don't be afraid to take this role seriously. The point of testing is to ensure the quality of the product, after all, and as a tester you are often in a unique position to intuitively sense when something is not right about a release. So even before you come to automation, or in the process of automating, it's always important to have your brain antennae tuned in this direction, ready to give feedback to the other members of the team.
3. Third, and related to knowing your users, you need to be able to imagine all the kinds of situations your users might get into it. This is particularly true when it comes to user input. Apps are often developed with a pretty narrow set of requirements, and developers often make assumptions about the kinds of input users might have. It's quite common for North American developers, for example, to never think about supporting unicode in username and password fields for example. But users based in many places around the world might need unicode support to adequately fill out forms in their language. Similarly, developers might not think about languages that write words from right to left, and the difference that makes for the UI. This kind of thing happens all the time. You never expect users to have more than 100 items in their cart, or for their checkout amount to be over 5 digits long, or whatever. As a tester, it's important to step back from what you consider normal and enter a more exploratory frame of mind where you consider the entire space of possibilities for how people might use the app. In the testing world, when we think of a very non-standard scenario that is nevertheless possible, we call that an "edge case". It's one that the developers might not have thought of to support or prohibit, and so could result in unexpected behavior for the user if they go down that particular road. It's your job to help think of these edge cases and turn them into tests. But this can be more difficult than it sounds, because there is an almost limitless number of edge cases for many apps. So you have to use your understanding of the app and of its users in order to make judgment calls about which edge cases are worth encoding in tests. We'll talk a bit more about how to think about doing that soon.
4. Fourth, it's important to make sure you have total clarity on the test requirements. In many cases, if you are given test requirements, they can turn out to be vague or ambiguous. But it will be your job to code them up in a set of automated testcases, where there is no room for ambiguity. In such situations, it's your job to clarify what is meant by the requirements. Often, one requirement handed on to you, for example, "test the login flow," will turn into a whole set of individual testcases, and only together will they actually amount to a real verification of the quality of the login flow.

Testing is really an art as well as a set of practices, and there are many more pointers to be learned along the way. But the main thing to remember is that you are not a robot, however much you are trying to program a software robot to test your app. And it's your responsibility to use your brain and think critically along multiple dimensions when it comes to testing your app. That is how you will provide value as a tester, not as a human cog in the big machine whose job is to simply turn requirements into automation. That latter role will almost certainly become the domain of robots themselves before too long. But the things we mentioned above (product knowledge, user empathy, imaginative exploration, and breaking down vague requirements into clearly-defined testcases) are not themselves going to be automated away anytime soon!

We just touched on some good conceptual advice for thinking like a tester. But when it comes to our test code, there's also a lot of practical advice it's worth paying attention to. But let's talk about it at a high level first.

Acting Like a Tester |
---- |
Use a solid test runner and framework. Don't reinvent the wheel (unless it's to learn how wheels work). Pytest is a great option for Python. Get to know your other testing tools and frameworks inside out, not just the Selenium and Appium commands. |
Get to know the entire software development cycle. Testing is just one part. You need to know where it fits in the overall picture. Learn about your team's CI server. Become a skilled "DevOps" who can set up testing systems from scratch as part of a CI pipeline. |
Use good software design patterns. Keep learning and improving your software skills. The best pattern is often situation-dependent, so get yourself in a lot of different situations. Develop software "wisdom" (not just skill) through time and practice. |

1. First, it always makes sense to use a good test runner and framework for your language. There are a number of technical concepts and features that are universal to all kinds of testing, that it doesn't make sense for you to implement on your own. For example, the concept of a test pass or a test failure is something that is very important to have in your test framework. So far in all of our hands-on examples in this course, we've just been writing automation scripts as Python files, where the automation is executed directly by calling Python from the command line. But how do we know if a given test has passed or failed? We have to inspect the command line output, and check for any exceptions raised or output that doesn't look right. This is not a good way to scale up our testsuite. Instead, we want to be able to run any number of tests, and get a report of which passed and which failed. This kind of thing is a feature which will be included in any decent test framework. There are lots of options for runners and frameworks, but in this course we are using Pytest, which is a great option for Python. Thus it's important for you to learn not just the ins and outs of the Appium and Selenium Python APIs--you'll also need to familiarize yourself with the APIs and usage patterns for whichever test framework you opt to use.

2. Second, we need to keep in mind that automated tests are not usually designed to be run in isolation. Testing is one part of a whole development cycle, and many other parts of this development cycle are also going to be automated. For the automated tests you write to be useful, they will need to be plugged into a pipeline within this system, usually called a Continuous Integration system. At the center of this system is something called a Continuous Integration server, or a CI server. For this reason you may need to get familiar with the CI server your team is using, or perhaps even be responsible for setting one up. We'll look at what it takes further on down the line, but all this to say, your job might entail getting your hands dirty with CI servers, figuring out how to deploy and run your testsuite as part of a CI server pipeline, and even figuring how to set up remote Appium and Selenium environments so your tests have devices and browsers to use! This is a lot, and in the old days it would have been the domain of the operations team to manage all of this. And maybe that's the way it is, or will be, for your situation too. But nowadays much of what used to be the responsibility of the ops team is not the responsibility of everybody based on the specific tasks they need to manage.

3. Third, it's extremely important that as you begin to build up a testsuite of automation code, that you use good software design patterns. By "patterns" here, I just mean ways of structuring your code consistently to solve certain types of problems. So far in this course I've not really used many patterns, other than the <code>try/finally</code> pattern we use to make sure the WebDriver session always gets stopped regardless of whether our test raises an exception. Instead I've been prioritizing superficial clarity and ease of readability at first glance. But these are not necessarily the qualities that lead to a good and robust testsuite which is easy to build and maintain as a team of multiple testers and developers. So if you want to be a good automated tester, it will be essential for you to keep improving in your software development practice. We'll talk about some specific patterns I recommend using, but there are many, many more to learn. But the trickiest part about good software design patterns is that they are often very situation-dependent. A pattern that is essential in one environment might be overkill in another. They also usually come with tradeoffs. Some patterns which lead to very concise code might also make it harder to follow before you've spent a long time with the codebase. We always need to understand what we're trying to optimize for in terms of the design of our testsuite, and choose patterns appropriately. For this reason, it's very difficult to teach design patterns outside of the context of an actual and evolving testsuite. Getting a good sense for which patterns to use simply takes time, practice, and lots of discussions with other team members about the various options and their benefits and drawbacks in a given situation.

All of these things matter a great deal when we try and take our automation knowledge and move into the domain of actually testing our apps in good and responsible ways. 



